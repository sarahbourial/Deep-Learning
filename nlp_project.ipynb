{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - ProjectÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"/Users/Boubou/Desktop/nlp_project/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build our World vectors class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's define our words vectors\n",
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())  \n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            \n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "            \n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        scrs = []\n",
    "        idc = []\n",
    "        for v in self.word2id:\n",
    "            scrs.append(self.score(w,v))\n",
    "            idc.append(v)# K most similar words: self.score  -  np.argsort \n",
    "        s = np.argsort(scrs)\n",
    "        f_s = []\n",
    "        \n",
    "        for i in range(4):\n",
    "            f_s.append(idc[s[-i-1]])\n",
    "        \n",
    "        \n",
    "        return f_s[:4]\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        #cosine similarity: np.dot  -  np.linalg.norm\n",
    "        #Returns the similarity score of two words\n",
    "        #Both words must be in the dictionary\n",
    "        \n",
    "        word_idx1 = self.word2vec[w1]\n",
    "        word_idx2 = self.word2vec[w2]\n",
    "        \n",
    "        dst = np.dot(word_idx1,word_idx2) / (np.linalg.norm(word_idx1) * np.linalg.norm(word_idx2))\n",
    "\n",
    "            \n",
    "        return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052403\n",
      "paris france 0.7775108541288558\n",
      "germany berlin 0.7420295235998394\n",
      "cat ['cat', 'cats', 'kitty', 'kitten']\n",
      "dog ['dog', 'dogs', 'puppy', 'Dog']\n",
      "dogs ['dogs', 'dog', 'pooches', 'Dogs']\n",
      "paris ['paris', 'france', 'Paris', 'london']\n",
      "germany ['germany', 'austria', 'europe', 'german']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=100000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w1, w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build our Bag of Words class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's define our bag of words vectors: \n",
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "\n",
    "#Let's encode our list of sentences: make them numpy array of sentence embeddings:\n",
    "    def encode(self, sentences, idf=False, fill_blank=False):\n",
    "       \n",
    "        sentemb = []\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                encod = [w2v.word2vec[w] for w in sent if w in w2v.word2vec]   #Let's look for the words embedings thanks to the function define above. \n",
    "                if encod != []:\n",
    "                    sentemb.append(np.mean(encod, axis = 0)) #If the words do exist in our dictionnary we can compute the sentence embeding by taking the mean of all the words embeding. \n",
    "            \n",
    "                else:\n",
    "                    sentemb.append([0]*300) #If the words are not in the dictionnary we assign the value 300. \n",
    "               \n",
    "                \n",
    "            else:\n",
    "                encod = [w2v.word2vec[w]*idf[w] for w in sent if w in w2v.word2vec and w in idf]\n",
    "                if encod != []:\n",
    "                    sentemb.append(np.mean(encod, axis = 0))\n",
    "                else:\n",
    "                    sentemb.append([0]*300)\n",
    "                # idf-weighted mean of word vectors\n",
    "                #sentemb.append(np.mean([w2v.word2vec[w]*idf[w] for w in sent if w in w2v.word2vec and w in idf], axis=0))\n",
    "                #assert False, 'TODO: fill in the blank'\n",
    "        \n",
    "        sentemb_np_array = np.asarray(sentemb)\n",
    "        \n",
    "        return np.vstack(sentemb_np_array)\n",
    "    \n",
    "#Let's define the function to get the most similar sentences:        \n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        index_sent = sentences.index(s)\n",
    "        \n",
    "        #Let's ncode all the sentences and retrieve the vector of the target sentence\n",
    "        keys = self.encode(sentences, idf)\n",
    "        query = keys[sentences.index(s),]  \n",
    "        \n",
    "        ##Let's normalize embeddings with np.linalg.norm\n",
    "        keys = keys / np.linalg.norm(keys, 2, 1)[:, None]  \n",
    "        query = query/np.linalg.norm(query)\n",
    "           \n",
    "        #Let's compute score with other sentences vector for s            \n",
    "        scores = np.dot(keys,query) \n",
    "        indx = scores.argsort()[::-1][1:K+1]\n",
    "        \n",
    "        #Let's select the K  most important score based on the most similar sentences\n",
    "        sentences_similar = []\n",
    "        scores_similar = []\n",
    "        \n",
    "        for i in indx:\n",
    "            sentences_similar.append(sentences[i])\n",
    "            scores_similar.append(scores[i])\n",
    "             \n",
    "        return  print( 'For ',s, ', the ',K,' most similar sentences are: \\n',sentences_similar,',\\n With respective similarities: \\n',scores_similar)\n",
    "\n",
    "   \n",
    "    #Let's define our score function to assess the similarity between sentences\n",
    "    def score(self, s1, s2, idf=False):\n",
    "    \n",
    "        #Let's compute the sentence embeding matrix\n",
    "        sentemb = self.encode(sentences,idf)\n",
    "        \n",
    "        #Let's encode the sentence we aim at comparing (s1 and s2)\n",
    "        s1_encod = sentemb[sentences.index(s1)]\n",
    "        s2_encod = sentemb[sentences.index(s2)]\n",
    "        \n",
    "        #Let's normalize s1 and s2 to do the dot product with np.linalg.norm\n",
    "        s1_normalized = np.linalg.norm(s1_encod)\n",
    "        s2_normalized = np.linalg.norm(s2_encod)\n",
    "        \n",
    "        # The cosine similarity is the dot-product of normalized vector \n",
    "        s = np.dot(s1_encod,s2_encod)/float(s1_normalized*s2_normalized)\n",
    "       \n",
    "        return print('The similarity score between \\n',s1,'\\n and ',s2,'\\n is up to ',s,'\\n')\n",
    "\n",
    "    \n",
    "   #Let's define our function to build the idf \n",
    "    def build_idf(self,sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {} \n",
    "\n",
    "        for sent in sentences:\n",
    "                for w in set(sent): #set() gives unique elements in list \n",
    "                    idf[w] = idf.get(w, 0) + 1\n",
    "\n",
    "        for w in idf:\n",
    "            idf[w] = max(1,(np.log10(len(sentences) / (idf[w]))))\n",
    "            \n",
    "        return idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try our Bag of Words class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 pretrained word vectors\n",
      "For  1 smiling african american boy .  , the  5  most similar sentences are: \n",
      " ['1 smiling african american boy . ', 'an african american male is singing into a microphone . ', 'an african american man is jumping in the air , while a boy claps . ', 'a man rock climbing in a forest . ', 'a man on a bmx bike is performing a trick . '] ,\n",
      " With respective similarities: \n",
      " [0.9999999999999998, 0.9937266333785353, 0.9937227698276596, 0.9936667419136049, 0.9933661492388242]\n",
      "The similarity score between \n",
      " 1 man singing and 1 man playing a saxophone in a concert .  \n",
      " and  10 people venture out to go crosscountry skiing .  \n",
      " is up to  0.955405396158266 \n",
      "\n",
      "For  1 smiling african american boy .  , the  5  most similar sentences are: \n",
      " ['a man spinning a skateboard in his hands while people watch . ', 'a man squatted at an outdoor campfire cooking food and smiling . ', 'a man squats near a fire and holds something into it . ', 'a man squats in the grass to take a picture of a flower while a slim woman looks on . ', 'a man sprinkling the corn on the ground . '] ,\n",
      " With respective similarities: \n",
      " [nan, nan, nan, nan, nan]\n",
      "The similarity score between \n",
      " 1 man singing and 1 man playing a saxophone in a concert .  \n",
      " and  10 people venture out to go crosscountry skiing .  \n",
      " is up to  nan \n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=5000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "sentences = open(\"/Users/Boubou/Desktop/nlp_project/data/sentences.txt\").read().split(\"\\n\")\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = {} if True else s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13])\n",
    "\n",
    "\n",
    "idf = {}  \n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "#Use Word2vec function defined above to load our French vectors:\n",
    "FR=Word2vec('/Users/Boubou/Desktop/nlp_project/wiki.fr.vec', nmax=50000)\n",
    "#Use Word2vec function defined above to load our French vectors:\n",
    "EN=Word2vec('/Users/Boubou/Desktop/nlp_project/wiki.en.vec', nmax=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "both_vocabs_words =[w for w in EN.word2vec if w in FR.word2vec ]\n",
    "\n",
    "#Let's define X, the Matrix with French words \n",
    "X = np.vstack([FR.word2vec[w] for w in both_vocabs_words])\n",
    "#Let's define Y, the Matrix with English words\n",
    "Y = np.vstack([EN.word2vec[w] for w in both_vocabs_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procustes solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "#Computation of the product Y.X^T with np.dot\n",
    "import numpy as np \n",
    "a=np.dot(X.transpose(),Y)\n",
    "\n",
    "# Computation of SVD(Y.X^T) with np.linalg.svd\n",
    "import numpy as np\n",
    "U, S, V_t= np.linalg.svd(a)\n",
    "\n",
    "#Computation of W = U.V^T\n",
    "W=np.dot(U,V_t)\n",
    "\n",
    "#Computation of W*French_vector with np.dot \n",
    "New_French_vector=np.dot(list(FR.word2vec.values()),W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'rabbit', 'hamster', 'feline']\n",
      "['kittens', 'kitten', 'puppy', 'fluffy']\n",
      "['dog', 'poodle', 'kittens', 'puppies']\n",
      "['france', 'auvergne', 'luxembourg', 'french']\n",
      "['berlin', 'munich', 'leipzig', 'hamburg']\n",
      "['shirt', 'maillots', 'shirts', 'pantalon']\n",
      "['chiens', 'dogs', 'chats', 'dog']\n",
      "['house', 'maison', 'houses', 'room']\n",
      "['country', 'countries', 'swaziland', 'pays']\n",
      "['paris', 'parisienne', 'lyon', 'versailles']\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "# Let's fill the french dictionary with New_French_vector created question 3\n",
    "fr_tr = dict()\n",
    "fr_word = list(FR.word2vec.keys())\n",
    "for i in range(len(New_French_vector)):\n",
    "    fr_tr[fr_word[i]] = New_French_vector[i]\n",
    "\n",
    "#Let's try to translate the words: 'chat', 'chaton', 'chienne', 'france', 'berlin' from French to English though cosine similarity    \n",
    "#Print the result from the most likely to the least (4 results)\n",
    "for w in ['chat', 'chaton', 'chienne', 'france', 'berlin']:\n",
    "    scores = {}\n",
    "    for word in EN.word2vec.keys():\n",
    "        v1 = fr_tr[w]\n",
    "        v2 = EN.word2vec[word]\n",
    "        scores[word] = np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    print(sorted(scores, key=scores.get, reverse=True)[0:4])  \n",
    "\n",
    "#Let's try to translate the words: 'shirt', 'dogs', 'house', 'country', 'paris' from English to French though cosine similarity    \n",
    "#Print the result from the most likely to the least (4 results)\n",
    "for w in ['shirt', 'dogs', 'house', 'country', 'paris']:\n",
    "    scores = {}\n",
    "    for word in fr_tr.keys():\n",
    "        v1 = EN.word2vec[w]\n",
    "        v2 = fr_tr[word]\n",
    "        scores[word] = np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    print(sorted(scores, key=scores.get, reverse=True)[0:4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see through the results that our algorithm manage to find the good translation in the first 2 words for everycase eventhough there can be some mistakes in the rest of the words the lexical remains really close. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "PATH_TO_SST = \"/Users//Boubou/Desktop/nlp_project/data/SST/\"\n",
    "\n",
    "\n",
    "#Let's load our sentences\n",
    "def load(path, label_cut=False):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            \n",
    "            if label_cut == False:\n",
    "                label, line = line.split(' ', 1)\n",
    "                labels.append(label)\n",
    "                \n",
    "            sentences.append(line.split())\n",
    "    return labels, sentences\n",
    "\n",
    "#Load the training data\n",
    "Y_train , X_train = load(os.path.join(PATH_TO_SST,'stsa.fine.train'), label_cut = False)\n",
    "\n",
    "#Load the dev data \n",
    "Y_dev , X_dev = load(os.path.join(PATH_TO_SST,'stsa.fine.dev'), label_cut = False)\n",
    "\n",
    "#Load the testing data\n",
    "Y_test , X_test = load(os.path.join(PATH_TO_SST,'stsa.fine.test.X'), label_cut = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "#Model without the idf weights with s2v\n",
    "idfq3 = {}\n",
    "Wtrain = s2v.encode(X_train)\n",
    "Wdev = s2v.encode(X_dev)\n",
    "Wtest = s2v.encode(X_test)\n",
    "\n",
    "#Let's generate the idf weights for encoding function\n",
    "idfq3 = s2v.build_idf(X_train)\n",
    "Wtrain_idf = s2v.encode(X_train ,idfq3)\n",
    "Wdev_idf = s2v.encode(X_dev, idfq3)\n",
    "Wtest_idf = s2v.encode(X_test, idfq3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import our packages to realise the Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoV-Mean\n",
      "----------------------------------------------\n",
      "The results for Simple LogisticRegression with the BoV-mean model \n",
      "CV (=5)  \n",
      " [0.38047925 0.37580362 0.37331773 0.37141183 0.37221571]\n",
      "The Cross Validation results mean are equal to 0.3746456296131087\n",
      "BoV-idf\n",
      "----------------------------------------------\n",
      "\n",
      " The Cross Validation results on the dev sets are equal to  0.3823796548592189 \n",
      "\n",
      "The results for Simple LogisticRegression with the BoV idf model \n",
      "CV (=5)  \n",
      " [0.36060783 0.36820573 0.35517847 0.36438196 0.36049238]\n",
      "The Cross Validation results idf are equal to  0.36177327254934605\n",
      "\n",
      "CV results on dev sets =  0.3651226158038147 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "\n",
    "#Let's define  our logistic regression function\n",
    "logiregr = LogisticRegression(penalty = 'l2',C=5)\n",
    "\n",
    "#Generate Cross validation results on training set with the BoV mean model\n",
    "Cross_Val_mean = cross_val_score(logiregr,Wtrain,Y_train,cv=5)\n",
    "print('BoV-Mean')\n",
    "print('----------------------------------------------')\n",
    "print('The results for Simple LogisticRegression with the BoV-mean model \\nCV (=5)  \\n',Cross_Val_mean)\n",
    "print('The Cross Validation results mean are equal to',np.mean(Cross_Val_mean))\n",
    "\n",
    "#Let's test with the dev set with BoV mean model\n",
    "logiregr.fit(Wtrain,Y_train)\n",
    "print('BoV-idf')\n",
    "print('----------------------------------------------')\n",
    "print('\\n The Cross Validation results on the dev sets are equal to ',logiregr.score(Wdev, Y_dev),'\\n')\n",
    "\n",
    "#Generate Cross validation results on training set with the BoV idf weight model\n",
    "Cross_Val_idf = cross_val_score(logiregr,Wtrain_idf,Y_train,cv=5)\n",
    "\n",
    "print('The results for Simple LogisticRegression with the BoV idf model \\nCV (=5)  \\n',Cross_Val_idf)\n",
    "print('The Cross Validation results idf are equal to ',np.mean(Cross_Val_idf))\n",
    "\n",
    "#Test with dev set with BoV idf model\n",
    "logiregr.fit(Wtrain_idf,Y_train)\n",
    "\n",
    "print('\\nCV results on dev sets = ',logiregr.score(Wdev_idf, Y_dev),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Boubou/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import Packaging to tune our Logistic Regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score is 0.3838951310861423\n",
      "The best parameters are {'C': 1}\n",
      "0.38419618528610355\n"
     ]
    }
   ],
   "source": [
    "# turn gs to True if you want to run the gridsearch again.\n",
    "gs = True\n",
    "\n",
    "#Make the gride seach on the logistic regression \n",
    "if gs:\n",
    "    parameters_grid = {\n",
    "                 'C': [0.001, 0.01, 0.1, 1, 5, 10, 100, 1000]\n",
    "                 }\n",
    "    logiregr2 = LogisticRegression()\n",
    "    crossvalidation = StratifiedKFold(Y_train, n_folds=5)\n",
    "\n",
    "    gridsearch = GridSearchCV(logiregr2,\n",
    "                               scoring='accuracy',\n",
    "                               param_grid=parameters_grid,\n",
    "                               cv=crossvalidation)\n",
    "\n",
    "    gridsearch.fit(Wtrain, Y_train)\n",
    "    logiregr2_best = gridsearch\n",
    "    parameters = gridsearch.best_params_\n",
    "\n",
    "    print('The best score is {}'.format(gridsearch.best_score_))\n",
    "    print('The best parameters are {}'.format(gridsearch.best_params_))\n",
    "else: \n",
    "    parameters = {'C': 1.0}\n",
    "    \n",
    "    logiregr2_best = LogisticRegression(**parameters)\n",
    "    logiregr2_best.fit(Wtrain, Y_train)\n",
    "    \n",
    "print(logiregr2_best.score(Wdev,Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "\n",
    "Y_pred_test = logiregr.predict(Wtest)\n",
    "\n",
    "text_file = open(\"logreg_bov_y_test_sst.txt\", \"w\")\n",
    "text_file.write('\\n'.join(Y_pred_test))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoV-mean model \n",
      "CV results for SVM [0.33138515 0.32963179 0.32358104 0.33099004 0.34114889]\n",
      "The cross validation mean is 0.3313473835977251 \n",
      "\n",
      "The cross validation results on dev sets are 0.34150772025431425 \n",
      "\n",
      "BoV-idf model \n",
      "CV results for SVM [0.29982466 0.31560491 0.27794032 0.31927358 0.30363423]\n",
      "The cross validation mean is  0.3032555401648246 \n",
      "\n",
      "The cross validation results on the dev sets are  0.335149863760218 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "GNB = GaussianNB()\n",
    "\n",
    "# Dev test \n",
    "\n",
    "CV_score = cross_val_score(GNB,Wtrain,Y_train,cv=5)\n",
    "print('BoV-mean model \\nCV results for SVM',CV_score)\n",
    "print('The cross validation mean is', np.mean(CV_score),'\\n')\n",
    "GNB.fit(Wtrain,Y_train)\n",
    "print('The cross validation results on dev sets are',GNB.score(Wdev, Y_dev),'\\n')\n",
    "\n",
    "Crossval_score_idf = cross_val_score(GNB,Wtrain_idf,Y_train,cv=5)\n",
    "print('BoV-idf model \\nCV results for SVM',Crossval_score_idf)\n",
    "print('The cross validation mean is ', np.mean(Crossval_score_idf),'\\n')\n",
    "GNB.fit(Wtrain_idf,Y_train)\n",
    "print('The cross validation results on the dev sets are ',GNB.score(Wdev_idf, Y_dev),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jennifervial/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "\n",
    "PATH_TO_SST = \"/Users/jennifervial/Downloads/nlp_project/data/SST/\"\n",
    "\n",
    "#Load the sentences like in the 1st exercise\n",
    "def load_texts(path, label_cut=False):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            if label_cut == False:\n",
    "                label, line = line.split(' ', 1)\n",
    "                labels.append(label)\n",
    "            sentences.append(line.split())\n",
    "    return labels, sentences\n",
    "\n",
    "#Load the training data \n",
    "Y_train , X_train = load_texts(os.path.join(PATH_TO_SST,'stsa.fine.train'), label_cut = False)\n",
    "\n",
    "#Load the dev data \n",
    "Y_dev , X_dev = load_texts(os.path.join(PATH_TO_SST,'stsa.fine.dev'), label_cut = False)\n",
    "\n",
    "#Load the testing data\n",
    "Y_test , X_test = load_texts(os.path.join(PATH_TO_SST,'stsa.fine.test.X'), label_cut = True)\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "Y_train = to_categorical(Y_train,5)\n",
    "Y_dev = to_categorical(Y_dev,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "#from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "def text_to_integers(text, size):\n",
    "    output=[]\n",
    "    for sent in text:\n",
    "        sent = str(sent)\n",
    "        new = keras.preprocessing.text.one_hot(sent, \n",
    "                                    size,\n",
    "                                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                     lower=True,\n",
    "                                     split=\" \")\n",
    "        output.append(new)\n",
    "    return output\n",
    "\n",
    "size_voc = 16000\n",
    "X_train_process = text_to_integers(X_train, size_voc)\n",
    "X_dev_process = text_to_integers(X_dev, size_voc)\n",
    "X_test_process = text_to_integers(X_test, size_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train_4 = pad_sequences(X_train_process)\n",
    "X_dev_4 = pad_sequences(X_dev_process)\n",
    "X_test_4 =pad_sequences(X_test_process)\n",
    "\n",
    "print(len(X_train_4[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jennifervial/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "word_embed_dim  = 2000  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = size_voc  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, word_embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 2000)        32000000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                528640    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 32,528,965\n",
      "Trainable params: 32,528,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'rmsprop' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jennifervial/anaconda3/lib/python3.6/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/4\n",
      "8544/8544 [==============================] - 204s 24ms/step - loss: 0.6433 - acc: 0.7639 - val_loss: 1.8365 - val_acc: 0.3887\n",
      "Epoch 2/4\n",
      "8544/8544 [==============================] - 194s 23ms/step - loss: 0.4310 - acc: 0.8516 - val_loss: 2.1188 - val_acc: 0.3678\n",
      "Epoch 3/4\n",
      "8544/8544 [==============================] - 220s 26ms/step - loss: 0.2944 - acc: 0.8998 - val_loss: 2.3923 - val_acc: 0.3424\n",
      "Epoch 4/4\n",
      "8544/8544 [==============================] - 236s 28ms/step - loss: 0.1987 - acc: 0.9340 - val_loss: 2.7016 - val_acc: 0.3669\n"
     ]
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "bs = 64\n",
    "n_epochs = 4\n",
    "\n",
    "LSTM = model.fit(X_train_4, Y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(X_dev_4, Y_dev))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train/dev accuracy and train/dev loss\n",
    "plt.figure(figsize=(15,5))\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(LSTM.history['acc'],'bo-' , label =\"Train\")\n",
    "plt.plot(LSTM.history['val_acc'], 'ro-' , label = \"Dev\")\n",
    "plt.title('Evolution of train/dev Accuracy in function of the number of epochs')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(LSTM.history['loss'],'bo-' , color='g', label =\"Train\")\n",
    "plt.plot(LSTM.history['val_loss'], 'ro-' , color='y', label = \"Dev\")\n",
    "plt.title(\"Evolution of train/dev Loss in function of the number of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "#Get the best Dev Accuracy\n",
    "print('\\n \\nBest Dev accuracy: %s'%np.max(LSTM.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "Y_test_pred = model.predict(X_test_4)\n",
    "Y_pred = []\n",
    "\n",
    "\n",
    "for i in range(Y_test_pred.shape[0]):\n",
    "    index_max = list(Y_test_pred[i]).index(max(Y_test_pred[i]))\n",
    "    Y_pred.append(str(index_max))\n",
    "Y_pred = np.asarray(Y_pred)\n",
    "\n",
    "#create file with predictions\n",
    "output_file = open(\"logreg_lstm_y_test_sst.txt\",'w')\n",
    "output_file.write('\\n'.join(Y_pred))\n",
    "output_file.close()\n",
    "\n",
    "#print(Y_test_pred[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jennifervial/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 2000)        32000000  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                528640    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 32,528,965\n",
      "Trainable params: 32,528,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jennifervial/anaconda3/lib/python3.6/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/4\n",
      "8544/8544 [==============================] - 594s 70ms/step - loss: 1.5138 - acc: 0.3221 - val_loss: 1.4086 - val_acc: 0.3742\n",
      "Epoch 2/4\n",
      "8544/8544 [==============================] - 424s 50ms/step - loss: 1.1461 - acc: 0.5256 - val_loss: 1.4650 - val_acc: 0.3860\n",
      "Epoch 3/4\n",
      "8544/8544 [==============================] - 423s 49ms/step - loss: 0.6963 - acc: 0.7383 - val_loss: 1.7589 - val_acc: 0.3660\n",
      "Epoch 4/4\n",
      "8544/8544 [==============================] - 475s 56ms/step - loss: 0.3649 - acc: 0.8766 - val_loss: 2.2820 - val_acc: 0.3606\n"
     ]
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 2000  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = size_voc  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Embedding(vocab_size, embed_dim))\n",
    "\n",
    "model2.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
    "\n",
    "model2.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'Adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model2.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model2.summary())\n",
    "\n",
    "bs = 64\n",
    "n_epochs = 4\n",
    "\n",
    "LSTM2 = model2.fit(X_train_4, Y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(X_dev_4, Y_dev))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train/dev accuracy and train/dev loss\n",
    "plt.figure(figsize=(15,5))\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(LSTM2.history['acc'],'bo-' , label =\"Train\")\n",
    "plt.plot(LSTM2.history['val_acc'], 'ro-' , label = \"Dev\")\n",
    "plt.title('Evolution of train/dev Accuracy in function of the number of epochs')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(LSTM2.history['loss'],'bo-' , color='g', label =\"Train\")\n",
    "plt.plot(LSTM2.history['val_loss'], 'ro-' , color='y', label = \"Dev\")\n",
    "plt.title(\"Evolution of train/dev Loss in function of the number of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "#Get the best Dev Accuracy\n",
    "print('\\n \\nBest Dev accuracy: %s'%np.max(LSTM.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file = open(\"XXX_XXX_y_test_sst.txt\",'w')\n",
    "output_file.write('\\n'.join(Y_pred))\n",
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
